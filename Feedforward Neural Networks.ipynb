{"cells":[{"cell_type":"markdown","id":"27916548","metadata":{"id":"27916548"},"source":["# **Artificial Neural Networks and Deep Learning**\n","\n","---\n","\n","## **Lecture 1: Feedforward Neural Networks**\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1MnEfJWqfX_yxIQroqZ6gh6adbSMeX91G\" width=\"500\"/>\n"]},{"cell_type":"markdown","source":["## üåê **Google Drive Connection**"],"metadata":{"id":"IfyKyjY0RaZZ"},"id":"IfyKyjY0RaZZ"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/gdrive\")\n","current_dir = \"/gdrive/My\\\\ Drive/[2025-2026]\\\\ AN2DL/Lecture\\\\ 1\"\n","%cd $current_dir"],"metadata":{"id":"LrXzSt7TRytm"},"id":"LrXzSt7TRytm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ‚öôÔ∏è **Libraries Import**"],"metadata":{"id":"sRzYlDZNR-in"},"id":"sRzYlDZNR-in"},{"cell_type":"code","source":["# Set seed for reproducibility\n","SEED = 42\n","\n","# Import necessary libraries\n","import os\n","\n","# Set environment variables before importing modules\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","# Suppress warnings\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","# Import necessary modules\n","import logging\n","import random\n","import numpy as np\n","\n","# Set seeds for random number generators in NumPy and Python\n","np.random.seed(SEED)\n","random.seed(SEED)\n","\n","# Import PyTorch\n","import torch\n","torch.manual_seed(SEED)\n","from torch import nn\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import TensorDataset, DataLoader\n","logs_dir = \"tensorboard\"\n","!pkill -f tensorboard\n","%load_ext tensorboard\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    torch.cuda.manual_seed_all(SEED)\n","    torch.backends.cudnn.benchmark = True\n","else:\n","    device = torch.device(\"cpu\")\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"Device: {device}\")\n","\n","# Import other libraries\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","# Configure plot display settings\n","sns.set(font_scale=1.4)\n","sns.set_style('white')\n","plt.rc('font', size=14)\n","%matplotlib inline"],"metadata":{"id":"MrZe8a1-SFc7"},"id":"MrZe8a1-SFc7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ‚è≥ **Data Loading**"],"metadata":{"id":"0NvMhVV_shHz"},"id":"0NvMhVV_shHz"},{"cell_type":"code","source":["# Load the Iris dataset into a variable called 'data'\n","data = load_iris()\n","\n","# Print the description of the Iris dataset\n","print(data.DESCR)"],"metadata":{"id":"u7bzPUzlSFac"},"id":"u7bzPUzlSFac","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*f6KbPXwksAliMIsibFyGJw.png\" width=\"800\">"],"metadata":{"id":"nlOnAjVjspxL"},"id":"nlOnAjVjspxL"},{"cell_type":"markdown","source":["## üîé **Exploration and Data Analysis**"],"metadata":{"id":"yLE3z2UyssJc"},"id":"yLE3z2UyssJc"},{"cell_type":"code","source":["# Create a DataFrame 'iris_dataset' from the Iris dataset\n","iris_dataset = pd.DataFrame(data.data, columns=data.feature_names)\n","print('Iris dataset shape', iris_dataset.shape)\n","\n","# Display the first 10 rows of the Iris dataset\n","iris_dataset.head(10)"],"metadata":{"id":"TfaSrKCjSFYB"},"id":"TfaSrKCjSFYB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the shape of the Iris dataset\n","print('Iris dataset shape', iris_dataset.shape)\n","\n","# Generate summary statistics for the Iris dataset\n","iris_dataset.describe()"],"metadata":{"id":"G5as_9Oks0oM"},"id":"G5as_9Oks0oM","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the target values from the Iris dataset\n","target = data.target\n","print('Target shape', target.shape)\n","\n","# Calculate the unique target labels and their counts\n","unique, count = np.unique(target, return_counts=True)\n","print('Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')"],"metadata":{"id":"nPzBW-QCs20d"},"id":"nPzBW-QCs20d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy the iris dataset\n","plot_dataset = iris_dataset.copy()\n","\n","# Assign target labels to the dataset\n","plot_dataset[\"Species\"] = target\n","\n","# Plot using seaborn pairplot\n","sns.pairplot(plot_dataset, hue=\"Species\", palette=\"tab10\", markers=[\"o\", \"s\", \"D\"])\n","plt.show()\n","\n","# Clean up by deleting the temporary dataset\n","del plot_dataset"],"metadata":{"id":"kdH_dm4Qs2x9"},"id":"kdH_dm4Qs2x9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Determine the number of features\n","input_features = iris_dataset.shape[1]\n","print(f'Number of input features: {input_features}')\n","\n","# Determine the number of classes\n","num_classes = len(np.unique(target))\n","print(f'Number of classes: {num_classes}')"],"metadata":{"id":"Zds7S52f6-oW"},"id":"Zds7S52f6-oW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üîÑ **Data Preprocessing**"],"metadata":{"id":"uIdpnIR6uQuF"},"id":"uIdpnIR6uQuF"},{"cell_type":"code","source":["# Prepare features and labels as float32 and int64 arrays\n","X = iris_dataset.values.astype(np.float32)\n","y = target.astype(np.int64)\n","\n","# First split: separate 20 samples for final testing\n","X_train_val, X_test, y_train_val, y_test = train_test_split(\n","    X,\n","    y,\n","    test_size=20,\n","    random_state=SEED,\n","    stratify=y\n",")\n","\n","# Second split: divide remaining data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train_val,\n","    y_train_val,\n","    test_size=20,\n","    random_state=SEED,\n","    stratify=y_train_val\n",")\n","\n","# Show final dataset sizes\n","print('Training set shape:\\t', X_train.shape, y_train.shape)\n","print('Validation set shape:\\t', X_val.shape, y_val.shape)\n","print('Test set shape:\\t\\t', X_test.shape, y_test.shape)"],"metadata":{"id":"nElhjaghs2vU"},"id":"nElhjaghs2vU","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find maximum value for each feature in training data\n","max_df = X_train.max(axis=0)\n","print('Iris dataset maximum values')\n","print(max_df)\n","\n","# Find minimum value for each feature in training data\n","min_df = X_train.min(axis=0)\n","print('\\nIris dataset minimum values')\n","print(min_df)"],"metadata":{"id":"99IrsLqks2s3"},"id":"99IrsLqks2s3","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply min-max scaling using training data statistics\n","X_train = (X_train - min_df) / (max_df - min_df)\n","X_val = (X_val - min_df) / (max_df - min_df)\n","X_test = (X_test - min_df) / (max_df - min_df)\n","\n","# Verify normalization worked (should be 0.0 to 1.0)\n","print(f\"New maximum values: {X_train.max(axis=0)}\")\n","print(f\"New minimum values: {X_train.min(axis=0)}\")"],"metadata":{"id":"x9HR40Sbs2qj"},"id":"x9HR40Sbs2qj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n","train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n","test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"],"metadata":{"id":"wTyQzj994p8B"},"id":"wTyQzj994p8B","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the batch size, which is the number of samples in each batch\n","BATCH_SIZE = 16"],"metadata":{"id":"-l7n37s5yDs2"},"id":"-l7n37s5yDs2","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_loader(ds, batch_size, shuffle, drop_last):\n","    # Determine optimal number of worker processes for data loading\n","    cpu_cores = os.cpu_count() or 2\n","    num_workers = max(2, min(4, cpu_cores))\n","\n","    # Create DataLoader with performance optimizations\n","    return DataLoader(\n","        ds,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers,\n","        pin_memory=True,  # Faster GPU transfer\n","        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n","        prefetch_factor=4,  # Load 4 batches ahead\n","    )"],"metadata":{"id":"kBwXFqcU3v-c"},"id":"kBwXFqcU3v-c","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create data loaders with different settings for each phase\n","train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n","test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"],"metadata":{"id":"K_5krlQdyDqV"},"id":"K_5krlQdyDqV","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9228d4b0"},"source":["# Get one batch from the training data loader\n","for xb, yb in train_loader:\n","    print(\"Features batch shape:\", xb.shape)\n","    print(\"Labels batch shape:\", yb.shape)\n","    break # Stop after getting one batch"],"id":"9228d4b0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üõ†Ô∏è **Model Building**"],"metadata":{"id":"hSLOyvU_mZaF"},"id":"hSLOyvU_mZaF"},{"cell_type":"code","source":["# Simple 3-layer neural network for classification\n","class FeedForwardNet(nn.Module):\n","    def __init__(self, in_features=input_features, hidden_size=16, num_classes=num_classes):\n","        super().__init__()\n","        # Architecture: input ‚Üí hidden ‚Üí output with ReLU activation\n","        self.net = nn.Sequential(\n","            nn.Linear(in_features, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Create model with 32 hidden units and move to GPU/CPU\n","model = FeedForwardNet(hidden_size=32).to(device)\n","model"],"metadata":{"id":"t0qPtTy3yDki"},"id":"t0qPtTy3yDki","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Display model architecture and parameter count\n","summary(model, input_size=(input_features,))"],"metadata":{"id":"h9yluw_kyDhn"},"id":"h9yluw_kyDhn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up TensorBoard logging and save model architecture\n","writer = SummaryWriter(\"./\"+logs_dir)\n","x = torch.randn(1, input_features).to(device)  # Create dummy input for graph visualization\n","writer.add_graph(model, x)\n","writer.close()"],"metadata":{"id":"hM4VvulsyDeb"},"id":"hM4VvulsyDeb","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"2TrlwcPzs2oN"},"id":"2TrlwcPzs2oN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üßÆ **Network Parameters**"],"metadata":{"id":"vTOhQi2Om67V"},"id":"vTOhQi2Om67V"},{"cell_type":"code","execution_count":null,"id":"d8d63a0a","metadata":{"id":"d8d63a0a"},"outputs":[],"source":["# Training configuration\n","LEARNING_RATE = 1e-3\n","EPOCHS = 500\n","\n","# Set up loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Enable mixed precision training for GPU acceleration\n","scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"]},{"cell_type":"markdown","source":["## üß† **Model Training**"],"metadata":{"id":"Slm4c3E-Cl2X"},"id":"Slm4c3E-Cl2X"},{"cell_type":"code","source":["def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device):\n","    \"\"\"\n","    Perform one complete training epoch through the entire training dataset.\n","\n","    Args:\n","        model (nn.Module): The neural network model to train\n","        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n","        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n","        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n","        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","\n","    Returns:\n","        tuple: (average_loss, accuracy) - Training loss and accuracy for this epoch\n","    \"\"\"\n","    model.train()  # Set model to training mode\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_targets = []\n","\n","    # Iterate through training batches\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Move data to device (GPU/CPU)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Clear gradients from previous step\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # Forward pass with mixed precision (if CUDA available)\n","        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","            logits = model(inputs)\n","            loss = criterion(logits, targets)\n","\n","        # Backward pass with gradient scaling\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Accumulate metrics\n","        running_loss += loss.item() * inputs.size(0)\n","        predictions = logits.argmax(dim=1)\n","        all_predictions.append(predictions.cpu().numpy())\n","        all_targets.append(targets.cpu().numpy())\n","\n","    # Calculate epoch metrics\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_accuracy = accuracy_score(\n","        np.concatenate(all_targets),\n","        np.concatenate(all_predictions)\n","    )\n","\n","    return epoch_loss, epoch_accuracy"],"metadata":{"id":"LfLk8gZoxopD"},"id":"LfLk8gZoxopD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_one_epoch(model, val_loader, criterion, device):\n","    \"\"\"\n","    Perform one complete validation epoch through the entire validation dataset.\n","\n","    Args:\n","        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n","        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n","        criterion (nn.Module): Loss function used to calculate validation loss\n","        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n","\n","    Returns:\n","        tuple: (average_loss, accuracy) - Validation loss and accuracy for this epoch\n","\n","    Note:\n","        This function automatically sets the model to evaluation mode and disables\n","        gradient computation for efficiency during validation.\n","    \"\"\"\n","    model.eval()  # Set model to evaluation mode\n","\n","    running_loss = 0.0\n","    all_predictions = []\n","    all_targets = []\n","\n","    # Disable gradient computation for validation\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            # Move data to device\n","            inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Forward pass with mixed precision (if CUDA available)\n","            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n","                logits = model(inputs)\n","                loss = criterion(logits, targets)\n","\n","            # Accumulate metrics\n","            running_loss += loss.item() * inputs.size(0)\n","            predictions = logits.argmax(dim=1)\n","            all_predictions.append(predictions.cpu().numpy())\n","            all_targets.append(targets.cpu().numpy())\n","\n","    # Calculate epoch metrics\n","    epoch_loss = running_loss / len(val_loader.dataset)\n","    epoch_accuracy = accuracy_score(\n","        np.concatenate(all_targets),\n","        np.concatenate(all_predictions)\n","    )\n","\n","    return epoch_loss, epoch_accuracy"],"metadata":{"id":"xAhwqTmGNZ1-"},"id":"xAhwqTmGNZ1-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def log_metrics_to_tensorboard(writer, epoch, train_loss, train_acc, val_loss, val_acc, model):\n","    \"\"\"\n","    Log training metrics and model parameters to TensorBoard for visualization.\n","\n","    Args:\n","        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n","        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n","        train_loss (float): Training loss for this epoch\n","        train_acc (float): Training accuracy for this epoch\n","        val_loss (float): Validation loss for this epoch\n","        val_acc (float): Validation accuracy for this epoch\n","        model (nn.Module): The neural network model (for logging weights/gradients)\n","\n","    Note:\n","        This function logs scalar metrics (loss/accuracy) and histograms of model\n","        parameters and gradients, which helps monitor training progress and detect\n","        issues like vanishing/exploding gradients.\n","    \"\"\"\n","    # Log scalar metrics\n","    writer.add_scalar('Loss/Training', train_loss, epoch)\n","    writer.add_scalar('Loss/Validation', val_loss, epoch)\n","    writer.add_scalar('Accuracy/Training', train_acc, epoch)\n","    writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n","\n","    # Log model parameters and gradients\n","    for name, param in model.named_parameters():\n","        if param.requires_grad:\n","            writer.add_histogram(f'{name}/weights', param.data, epoch)\n","            if param.grad is not None:\n","                writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"],"metadata":{"id":"ymEQkoYBNbXj"},"id":"ymEQkoYBNbXj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# Initialise metrics tracking\n","training_history = {\n","    'train_loss': [], 'val_loss': [],\n","    'train_acc': [], 'val_acc': []\n","}\n","print_frequency = 10\n","\n","print(f\"Training {EPOCHS} epochs...\")\n","\n","# Main training loop: iterate through epochs\n","for epoch in range(1, EPOCHS + 1):\n","\n","    # Forward pass through training data, compute gradients, update weights\n","    train_loss, train_accuracy = train_one_epoch(\n","        model, train_loader, criterion, optimizer, scaler, device\n","    )\n","\n","    # Evaluate model on validation data without updating weights\n","    val_loss, val_accuracy = validate_one_epoch(\n","        model, val_loader, criterion, device\n","    )\n","\n","    # Store metrics for plotting and analysis\n","    training_history['train_loss'].append(train_loss)\n","    training_history['val_loss'].append(val_loss)\n","    training_history['train_acc'].append(train_accuracy)\n","    training_history['val_acc'].append(val_accuracy)\n","\n","    # Write metrics to TensorBoard for visualisation\n","    log_metrics_to_tensorboard(\n","        writer, epoch, train_loss, train_accuracy, val_loss, val_accuracy, model\n","    )\n","\n","    # Print progress every N epochs or on first epoch\n","    if epoch % print_frequency == 0 or epoch == 1:\n","        print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n","              f\"Train: Loss={train_loss:.4f}, Acc={train_accuracy:.4f} | \"\n","              f\"Val: Loss={val_loss:.4f}, Acc={val_accuracy:.4f}\")\n","\n","# Save trained model state dict and close TensorBoard writer\n","torch.save(model.state_dict(), 'model.pt')\n","writer.close()\n","print(\"Model saved: 'model.pt'\")"],"metadata":{"id":"k4LICFlA-PVR"},"id":"k4LICFlA-PVR","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"660e252d","metadata":{"id":"660e252d"},"outputs":[],"source":["# Create a figure with two side-by-side subplots (two columns)\n","fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 5))\n","\n","# Plot of training and validation loss on the first axis\n","ax1.plot(training_history['train_loss'], label='Training loss', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax1.plot(training_history['val_loss'], label='Validation loss', alpha=0.9, color='#ff7f0e')\n","ax1.set_title('Loss')\n","ax1.legend()\n","ax1.grid(alpha=0.3)\n","\n","# Plot of training and validation accuracy on the second axis\n","ax2.plot(training_history['train_acc'], label='Training accuracy', alpha=0.3, color='#ff7f0e', linestyle='--')\n","ax2.plot(training_history['val_acc'], label='Validation accuracy', alpha=0.9, color='#ff7f0e')\n","ax2.set_title('Accuracy')\n","ax2.legend()\n","ax2.grid(alpha=0.3)\n","\n","# Adjust the layout and display the plot\n","plt.tight_layout()\n","plt.subplots_adjust(right=0.85)\n","plt.show()"]},{"cell_type":"code","source":["# Copy TensorBoard logs to accessible location for Colab\n","!rsync -a $current_dir\"/\"$logs_dir/ \"/content/\"$logs_dir/\n","\n","# Launch TensorBoard interface\n","%tensorboard --logdir \"/content/\"$logs_dir"],"metadata":{"id":"HyuonUg6opra"},"id":"HyuonUg6opra","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## üïπÔ∏è **Inference**\n","\n","\n","\n"],"metadata":{"id":"p_3j935nGGZG"},"id":"p_3j935nGGZG"},{"cell_type":"code","source":["# Load trained model and set to evaluation mode\n","best_model = FeedForwardNet(hidden_size=32).to(device)\n","best_model.load_state_dict(torch.load('model.pt', map_location=device))\n","best_model.eval()"],"metadata":{"id":"gP5Dj8A5KPlZ"},"id":"gP5Dj8A5KPlZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Collect predictions and ground truth labels\n","test_preds, test_targets = [], []\n","with torch.no_grad():  # Disable gradient computation for inference\n","    for xb, yb in test_loader:\n","        xb = xb.to(device)\n","\n","        # Forward pass: get model predictions\n","        logits = best_model(xb)\n","        preds = logits.argmax(dim=1).cpu().numpy()\n","\n","        # Store batch results\n","        test_preds.append(preds)\n","        test_targets.append(yb.numpy())\n","\n","# Combine all batches into single arrays\n","test_preds = np.concatenate(test_preds)\n","test_targets = np.concatenate(test_targets)"],"metadata":{"id":"SuSUXSjtKRez"},"id":"SuSUXSjtKRez","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8da5e5a9","metadata":{"id":"8da5e5a9"},"outputs":[],"source":["# Calculate overall test accuracy\n","test_acc = accuracy_score(test_targets, test_preds)\n","print(f\"Accuracy over the test set: {test_acc:.4f}\")\n","\n","# Generate confusion matrix for detailed error analysis\n","cm = confusion_matrix(test_targets, test_preds)\n","\n","# Create numeric labels for heatmap annotation\n","labels = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n","\n","# Visualise confusion matrix\n","plt.figure(figsize=(8, 7))\n","sns.heatmap(cm, annot=labels, fmt='',\n","            xticklabels=data.target_names,\n","            yticklabels=data.target_names,\n","            cmap='Blues')\n","plt.xlabel('Predicted labels')\n","plt.ylabel('True labels')\n","plt.title('Confusion Matrix ‚Äî Test Set')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","source":["## üìú **Homework Exercise**\n","Perform the same classification analysis on the Penguins dataset to predict the correct species. Pay attention to missing values!\n","\n","<img src=\"https://allisonhorst.github.io/palmerpenguins/reference/figures/lter_penguins.png\" width=\"800\">\n","\n","```\n","# Data Loading\n","os.environ[\"DATASET_NAME\"] = \"penguins.csv\"\n","os.environ[\"DATASET_URL\"] = \"1qn1P6_KW08wGRfSkTlzBoDCyVb18T3Lk\"\n","if not os.path.exists(os.environ[\"DATASET_NAME\"]):\n","    print(\"Downloading data...\")\n","    ! gdown -q ${DATASET_URL}\n","    print(\"Download completed\")\n","else:\n","    print(\"Data already downloaded. Using cached data...\")\n","dataset = pd.read_csv('penguins.csv')\n","```"],"metadata":{"id":"o7pfoH3lKXOq"},"id":"o7pfoH3lKXOq"},{"cell_type":"markdown","source":["#  \n","<img src=\"https://airlab.deib.polimi.it/wp-content/uploads/2019/07/airlab-logo-new_cropped.png\" width=\"350\">\n","\n","##### Connect with us:\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/LinkedIn_icon.svg/2048px-LinkedIn_icon.svg.png\" width=\"14\"> **LinkedIn:**  [AIRLab Polimi](https://www.linkedin.com/company/airlab-polimi/)\n","- <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Instagram_logo_2022.svg/800px-Instagram_logo_2022.svg.png\" width=\"14\"> **Instagram:** [airlab_polimi](https://www.instagram.com/airlab_polimi/)\n","\n","##### Contributors:\n","- **Eugenio Lomurno**: eugenio.lomurno@polimi.it\n","- **Alberto Archetti**: alberto.archetti@polimi.it\n","- **Roberto Basla**: roberto.basla@polimi.it\n","- **Carlo Sgaravatti**: carlo.sgaravatti@polimi.it\n","\n","```\n","   Copyright 2025 Eugenio Lomurno, Alberto Archetti, Roberto Basla, Carlo Sgaravatti\n","\n","   Licensed under the Apache License, Version 2.0 (the \"License\");\n","   you may not use this file except in compliance with the License.\n","   You may obtain a copy of the License at\n","\n","       http://www.apache.org/licenses/LICENSE-2.0\n","\n","   Unless required by applicable law or agreed to in writing, software\n","   distributed under the License is distributed on an \"AS IS\" BASIS,\n","   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","   See the License for the specific language governing permissions and\n","   limitations under the License.\n","```"],"metadata":{"id":"0_8hiKnlmABA"},"id":"0_8hiKnlmABA"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}